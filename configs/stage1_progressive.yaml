# ============================================================================
# Progressive TTQ Quantization - All 3 Stages Defined (FIXED)
# ============================================================================
# This config defines a 3-stage progressive quantization strategy for YOLO11n
#
# CRITICAL FIX: threshold now represents multiplier for MEAN, not MAX
#   OLD: threshold=0.05 → delta = 0.05 * max(|W|) → 60% zeros → mAP drop 79%
#   NEW: threshold=0.7  → delta = 0.7 * mean(|W|) → 35% zeros → much better!
#
# Usage:
#   Stage 1: python train.py --config configs/progressive_quantization.yaml --stage 1
#   Stage 2: python train.py --config configs/progressive_quantization.yaml --stage 2
#   Stage 3: python train.py --config configs/progressive_quantization.yaml --stage 3
#
# Expected Results (with mean-based threshold):
#   Stage 1: Quantize shallow layers (15-20% of model) - Expected mAP: 0.75-0.80
#   Stage 2: Quantize middle layers (35-45% of model) - Expected mAP: 0.65-0.75
#   Stage 3: Quantize deep layers (60-65% of model)   - Expected mAP: 0.50-0.65
# ============================================================================


# ============================================================================
# STAGE DEFINITIONS
# ============================================================================
stages:
  # Stage 1: Quantize shallow backbone layers only
  stage1:
    name: 'stage1_progressive_shallow'
    description: 'Progressive quantization - Stage 1: Shallow layers only (backbone entry)'
    
    model:
      weights: 'yolo11n.pt'  # Start from pretrained FP32
    
    quantization:
      threshold: 0.7  # FIXED: 0.7 * mean(|W|) from TWN paper (~35% zeros)
      # Alternative values to experiment:
      #   threshold: 0.5  → More zeros (~45%), more compression, lower mAP
      #   threshold: 0.9  → Fewer zeros (~25%), less compression, higher mAP
      quantize_first_layer: false
      target_layers: [2, 3, 4, 5]  # Shallow backbone: layers 2-5
      # Expected: ~12-15 layers quantized out of 88 total Conv2d layers
    
    train:
      epochs: 20
      batch: 8
      imgsz: 640
      lr0: 0.001  # Standard LR for initial quantization
      weight_decay: 0.0005
      momentum: 0.937
      optimizer: 'Adam'
      workers: 8
      device: 0
    
    val:
      batch: 16
      imgsz: 640
    
    logging:
      name: 'stage1_progressive_shallow'
      save_period: 10
      project: 'ttq_checkpoints'
  
  
  # Stage 2: Add middle layers to quantization
  stage2:
    name: 'stage2_progressive_middle'
    description: 'Progressive quantization - Stage 2: Add middle layers (backbone + neck entry)'
    
    model:
      # IMPORTANT: Load from Stage 1 checkpoint
      weights: 'ttq_checkpoints/stage1_progressive_shallow/weights/best.pt'
    
    quantization:
      threshold: 0.7  # FIXED: Mean-based threshold
      quantize_first_layer: false
      target_layers: [2, 3, 4, 5, 6, 7, 8, 9, 13]  # Add layers 6-9, 13
      # Expected: ~28-32 layers quantized out of 88 total Conv2d layers
    
    train:
      epochs: 50
      batch: 8
      imgsz: 640
      lr0: 0.0005  # Lower LR for fine-tuning
      weight_decay: 0.0005
      momentum: 0.937
      optimizer: 'Adam'
      workers: 8
      device: 0
    
    val:
      batch: 16
      imgsz: 640
    
    logging:
      name: 'stage2_progressive_middle'
      save_period: 10
      project: 'ttq_checkpoints'
  
  
  # Stage 3: Add deep layers (nearly full quantization)
  stage3:
    name: 'stage3_progressive_deep'
    description: 'Progressive quantization - Stage 3: Add deep layers (backbone + full neck)'
    
    model:
      # IMPORTANT: Load from Stage 2 checkpoint
      weights: 'ttq_checkpoints/stage2_progressive_middle/weights/best.pt'
    
    quantization:
      threshold: 0.7  # FIXED: Mean-based threshold
      quantize_first_layer: false
      target_layers: [2, 3, 4, 5, 6, 7, 8, 9, 13, 16, 17, 19, 20, 22]
      # Add layers: 16, 17, 19, 20, 22 (neck layers)
      # Expected: ~50-55 layers quantized out of 88 total Conv2d layers
      # Note: Still excluding detect head (23) and C2PSA attention (10)
    
    train:
      epochs: 100  # More epochs for final convergence
      batch: 8
      imgsz: 640
      lr0: 0.0003  # Even lower LR for final fine-tuning
      weight_decay: 0.0005
      momentum: 0.937
      optimizer: 'Adam'
      workers: 8
      device: 0
    
    val:
      batch: 16
      imgsz: 640
    
    logging:
      name: 'stage3_progressive_deep'
      save_period: 10
      project: 'ttq_checkpoints'


# ============================================================================
# DATA SETTINGS (shared across all stages)
# ============================================================================
data:
  train: 'coco8.yaml'  # For testing - change to 'coco128.yaml' or 'coco.yaml' for real training
  val: 'coco8.yaml'


# ============================================================================
# THRESHOLD REFERENCE (MEAN-BASED)
# ============================================================================
# threshold: Multiplier for mean(|W|) to compute delta
#
# Formula: delta = threshold * mean(|W|)
#   threshold=0.5 → delta = 0.5 * mean(|W|) → ~45% zeros (aggressive)
#   threshold=0.7 → delta = 0.7 * mean(|W|) → ~35% zeros (recommended, from TWN paper)
#   threshold=0.9 → delta = 0.9 * mean(|W|) → ~25% zeros (conservative)
#
# OLD (max-based): threshold=0.05 → delta = 0.05 * max(|W|) → 60% zeros → BROKEN
# NEW (mean-based): threshold=0.7 → delta = 0.7 * mean(|W|) → 35% zeros → WORKS
#
# Zero percentage directly impacts accuracy:
#   60% zeros → mAP drops 79% (0.85 → 0.18) ← BROKEN
#   35% zeros → mAP drops 15-20% (0.85 → 0.68-0.72) ← ACCEPTABLE
# ============================================================================


# ============================================================================
# LAYER INDEX REFERENCE (for YOLO11n architecture)
# ============================================================================
# Layer Index | Module Type       | Description
# ------------|-------------------|------------------------------------------
# 0           | Conv              | Stem (first layer) - EXCLUDED
# 1-2         | Conv + C3k2       | Early backbone
# 3-5         | Conv + C3k2       | Shallow backbone
# 6-9         | Conv + C3k2 + Conv| Middle backbone
# 10          | C2PSA             | Attention block - EXCLUDED
# 11-13       | SPPF + C3k2       | Late backbone
# 16-17       | Upsample + Concat | Neck (P4)
# 19-20       | Conv + C3k2       | Neck (P3)
# 22          | C3k2              | Final neck layer
# 23          | Detect            | Detection head - EXCLUDED
# 
# Progressive Strategy:
#   Stage 1: [2, 3, 4, 5]                         → ~15 layers (17% of Conv2d)
#   Stage 2: [2-9, 13]                           → ~32 layers (36% of Conv2d)
#   Stage 3: [2-9, 13, 16, 17, 19, 20, 22]      → ~55 layers (62% of Conv2d)
# ============================================================================
